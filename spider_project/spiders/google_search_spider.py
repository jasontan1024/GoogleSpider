import scrapy
from scrapy_playwright.page import PageMethod
from spider_project.items import GoogleSearchItem
from urllib.parse import quote_plus, urlparse, parse_qs, urlencode, urlunparse
import os
from datetime import datetime
import json
from pathlib import Path


class GoogleSearchSpider(scrapy.Spider):
    """
    è°·æ­Œæœç´¢ç»“æœçˆ¬è™«
    
    ä½¿ç”¨ Playwright å¤„ç† JavaScript æ¸²æŸ“çš„æœç´¢ç»“æœé¡µé¢
    æ”¶é›†æ ‡é¢˜ã€URL å’Œæè¿°ä¿¡æ¯
    """
    name = 'google_search'
    allowed_domains = ['google.com', 'www.google.com']
    
    # ä»ç¯å¢ƒå˜é‡è·å–æœç´¢å…³é”®è¯ï¼Œé»˜è®¤ä¸º 'python scrapy'
    search_query = os.getenv('SEARCH_QUERY', 'python scrapy')
    max_pages = int(os.getenv('MAX_PAGES', '3'))  # æœ€å¤šçˆ¬å–é¡µæ•°
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        base_path = Path(__file__).parent.parent
        js_path = base_path / 'js'
        
        # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¿…é¡»ï¼Œæ— é»˜è®¤é…ç½®ï¼‰
        config_path = base_path / 'config.json'
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
            self.logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}") from e
        
        # åŠ è½½ JavaScript æå–è„šæœ¬ï¼ˆå¿…é¡»ï¼‰
        extractor_path = js_path / 'extractors.js'
        if not extractor_path.exists():
            raise FileNotFoundError(f"JavaScript æå–å™¨ä¸å­˜åœ¨: {extractor_path}")
        
        try:
            with open(extractor_path, 'r', encoding='utf-8') as f:
                self.extractor_js = f.read()
            self.logger.info(f"å·²åŠ è½½ JavaScript æå–å™¨: {extractor_path}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½ JavaScript æå–å™¨å¤±è´¥: {e}") from e
        
        # åŠ è½½å·¥å…·å‡½æ•°è„šæœ¬ï¼ˆå¯é€‰ï¼‰
        utils_path = js_path / 'utils.js'
        try:
            if utils_path.exists():
                with open(utils_path, 'r', encoding='utf-8') as f:
                    self.utils_js = f.read()
            else:
                self.utils_js = None
        except Exception as e:
            self.logger.warning(f"åŠ è½½å·¥å…·å‡½æ•°è„šæœ¬å¤±è´¥: {e}")
            self.utils_js = None
        
        # åŠ è½½äººç±»è¡Œä¸ºæ¨¡æ‹Ÿè„šæœ¬ï¼ˆå¯é€‰ï¼‰
        human_behavior_path = js_path / 'human_behavior.js'
        try:
            if human_behavior_path.exists():
                with open(human_behavior_path, 'r', encoding='utf-8') as f:
                    self.human_behavior_js = f.read()
            else:
                self.human_behavior_js = None
        except Exception as e:
            self.logger.warning(f"åŠ è½½äººç±»è¡Œä¸ºæ¨¡æ‹Ÿè„šæœ¬å¤±è´¥: {e}")
            self.human_behavior_js = None
        
        # åŠ è½½åæ£€æµ‹è„šæœ¬
        stealth_init_path = js_path / 'stealth.js'
        stealth_after_path = js_path / 'stealth_after.js'
        
        try:
            if stealth_init_path.exists():
                with open(stealth_init_path, 'r', encoding='utf-8') as f:
                    self.stealth_init_js = f.read()
            else:
                self.stealth_init_js = None
                self.logger.warning(f"åæ£€æµ‹è„šæœ¬ä¸å­˜åœ¨: {stealth_init_path}")
        except Exception as e:
            self.logger.warning(f"åŠ è½½åæ£€æµ‹è„šæœ¬å¤±è´¥: {e}")
            self.stealth_init_js = None
        
        try:
            if stealth_after_path.exists():
                with open(stealth_after_path, 'r', encoding='utf-8') as f:
                    self.stealth_after_js = f.read()
            else:
                self.stealth_after_js = None
                self.logger.warning(f"åæ£€æµ‹è„šæœ¬ä¸å­˜åœ¨: {stealth_after_path}")
        except Exception as e:
            self.logger.warning(f"åŠ è½½åæ£€æµ‹è„šæœ¬å¤±è´¥: {e}")
            self.stealth_after_js = None
    
    def start_requests(self):
        """ç”Ÿæˆåˆå§‹æœç´¢è¯·æ±‚"""
        # æ„å»ºè°·æ­Œæœç´¢ URL
        search_url = f"https://www.google.com/search?q={quote_plus(self.search_query)}&hl=en"
        
        self.logger.info(f"å¼€å§‹æœç´¢: {self.search_query}")
        
        yield scrapy.Request(
            url=search_url,
            callback=self.parse,
            meta={
                "playwright": True,
                "playwright_include_page": True,
                "playwright_page_methods": [
                    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                    PageMethod("wait_for_load_state", "networkidle", timeout=60000),
                ],
            },
            dont_filter=True
        )
    
    async def parse(self, response):
        """è§£ææœç´¢ç»“æœé¡µé¢"""
        page = response.meta.get("playwright_page")
        page_number = response.meta.get("page_number", 1)
        
        try:
            # åœ¨é¡µé¢åŠ è½½å‰æ³¨å…¥åæ£€æµ‹è„šæœ¬
            if self.stealth_init_js:
                await page.add_init_script(self.stealth_init_js)
            
            # æ³¨å…¥äººç±»è¡Œä¸ºæ¨¡æ‹Ÿè„šæœ¬
            if self.human_behavior_js:
                await page.add_init_script(self.human_behavior_js)
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            self.logger.info(f"ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ...")
            await page.wait_for_load_state("networkidle", timeout=60000)
            
            # æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼šéšæœºç­‰å¾…å’Œæ»šåŠ¨ï¼ˆå¢åŠ ç­‰å¾…æ—¶é—´ä»¥é™ä½é¢‘ç‡ï¼‰
            import random
            initial_wait = 5000 + random.randint(0, 5000)  # 5-10ç§’éšæœºç­‰å¾…
            self.logger.info(f"é¡µé¢åŠ è½½åç­‰å¾… {initial_wait/1000:.1f} ç§’...")
            await page.wait_for_timeout(initial_wait)
            
            # æ¨¡æ‹Ÿé¼ æ ‡ç§»åŠ¨å’Œæ»šåŠ¨ï¼ˆå¢åŠ ç­‰å¾…æ—¶é—´ä»¥é™ä½é¢‘ç‡ï¼‰
            try:
                # éšæœºæ»šåŠ¨
                scroll_amount = random.randint(100, 500)
                await page.evaluate(f"window.scrollBy(0, {scroll_amount});")
                scroll_wait1 = 2000 + random.randint(0, 3000)  # 2-5ç§’ç­‰å¾…
                await page.wait_for_timeout(scroll_wait1)
                
                # ç»§ç»­æ»šåŠ¨
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 3);")
                scroll_wait2 = 2000 + random.randint(0, 3000)  # 2-5ç§’ç­‰å¾…
                await page.wait_for_timeout(scroll_wait2)
                
                # æ»šåŠ¨å›é¡¶éƒ¨é™„è¿‘
                await page.evaluate("window.scrollTo(0, 100);")
                scroll_wait3 = 3000 + random.randint(0, 5000)  # 3-8ç§’ç­‰å¾…
                await page.wait_for_timeout(scroll_wait3)
                total_wait = initial_wait + scroll_wait1 + scroll_wait2 + scroll_wait3
                self.logger.info(f"é¡µé¢äº¤äº’å®Œæˆï¼Œæ€»ç­‰å¾…æ—¶é—´çº¦ {total_wait/1000:.1f} ç§’")
            except Exception as e:
                self.logger.debug(f"æ¨¡æ‹Ÿæ»šåŠ¨æ—¶å‡ºé”™: {e}")
            
            # æ³¨å…¥æ›´å¤šåæ£€æµ‹è„šæœ¬
            if self.stealth_after_js:
                try:
                    await page.evaluate(self.stealth_after_js)
                except Exception as e:
                    self.logger.warning(f"æ³¨å…¥åæ£€æµ‹è„šæœ¬å¤±è´¥ï¼ˆé¡µé¢å¯èƒ½å·²å…³é—­ï¼‰: {e}")
                    # æ£€æŸ¥é¡µé¢æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                    if page.is_closed():
                        self.logger.error("é¡µé¢å·²è¢«å…³é—­ï¼Œæ— æ³•ç»§ç»­")
                        return
            
            # ä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿è°ƒè¯•ï¼ˆæ€»æ˜¯ä¿å­˜ç¬¬ä¸€é¡µï¼‰
            if page_number == 1:
                try:
                    # æ£€æŸ¥logsç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
                    logs_dir = Path(__file__).parent.parent.parent / "logs"
                    logs_dir.mkdir(exist_ok=True)
                    
                    screenshot_path = logs_dir / "page_screenshot.png"
                    html_path = logs_dir / "page_content.html"
                    
                    await page.screenshot(path=str(screenshot_path), full_page=True)
                    html_content = await page.content()
                    with open(html_path, "w", encoding="utf-8") as f:
                        f.write(html_content)
                    self.logger.info(f"å·²ä¿å­˜é¡µé¢æˆªå›¾å’ŒHTMLåˆ° {logs_dir}/ ç›®å½•")
                except Exception as e:
                    self.logger.warning(f"ä¿å­˜è°ƒè¯•æ–‡ä»¶å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç æˆ–å…¶ä»–æ‹¦æˆªé¡µé¢
            page_title = await page.title()
            # ä½¿ç”¨å·¥å…·å‡½æ•°è·å–é¡µé¢æ–‡æœ¬
            if self.utils_js:
                page_text = await page.evaluate(f"{self.utils_js}\ngetPageText();")
            else:
                # å¦‚æœå·¥å…·å‡½æ•°æœªåŠ è½½ï¼Œä½¿ç”¨ç®€å•æ–¹å¼
                page_text = await page.evaluate("document.body.innerText")
            page_url = page.url
            
            self.logger.info(f"é¡µé¢æ ‡é¢˜: {page_title}")
            self.logger.info(f"é¡µé¢URL: {page_url}")
            
            # æ›´å®½æ¾çš„éªŒè¯ç æ£€æµ‹ - ä»é…ç½®ä¸­è¯»å–
            captcha_indicators = self.config['validation']['captcha_indicators']
            has_captcha = any(indicator in page_text.lower() for indicator in captcha_indicators)
            
            if has_captcha:
                self.logger.warning("âš ï¸  æ£€æµ‹åˆ°éªŒè¯ç é¡µé¢ï¼")
                self.logger.warning("ğŸ“Œ è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ç ")
                self.logger.warning("â³ ç­‰å¾…60ç§’ï¼Œè¯·åœ¨æ­¤æœŸé—´å®ŒæˆéªŒè¯ç ...")
                
                # ç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯ç ï¼ˆæœ€å¤š60ç§’ï¼‰
                import asyncio
                max_wait_time = 60  # æœ€å¤šç­‰å¾…60ç§’
                check_interval = 3  # æ¯3ç§’æ£€æŸ¥ä¸€æ¬¡
                waited_time = 0
                
                while waited_time < max_wait_time:
                    await asyncio.sleep(check_interval)
                    waited_time += check_interval
                    
                    # æ£€æŸ¥é¡µé¢æ˜¯å¦å·²ä¸å†æ˜¯éªŒè¯ç é¡µé¢
                    try:
                        current_url = page.url
                        current_title = await page.title()
                        current_text = ""
                        if self.utils_js:
                            current_text = await page.evaluate(f"{self.utils_js}\ngetPageText();")
                        else:
                            current_text = await page.evaluate("document.body.innerText")
                        
                        # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨éªŒè¯ç é¡µé¢
                        still_captcha = any(indicator in current_text.lower() for indicator in captcha_indicators) or "sorry" in current_url.lower()
                        
                        if not still_captcha:
                            self.logger.info(f"âœ… éªŒè¯ç å·²å®Œæˆï¼ç­‰å¾…äº† {waited_time} ç§’")
                            # ç­‰å¾…é¡µé¢ç¨³å®š
                            await page.wait_for_load_state("networkidle", timeout=10000)
                            await page.wait_for_timeout(2000)
                            break
                        else:
                            remaining = max_wait_time - waited_time
                            if remaining > 0 and waited_time % 10 == 0:  # æ¯10ç§’æç¤ºä¸€æ¬¡
                                self.logger.info(f"â³ ä»åœ¨ç­‰å¾…éªŒè¯ç å®Œæˆ... å‰©ä½™ {remaining} ç§’")
                    except Exception as e:
                        self.logger.debug(f"æ£€æŸ¥éªŒè¯ç çŠ¶æ€æ—¶å‡ºé”™: {e}")
                
                if waited_time >= max_wait_time:
                    self.logger.warning("â° ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å°è¯•æå–æ•°æ®...")
                else:
                    self.logger.info("âœ… éªŒè¯ç å·²å®Œæˆï¼Œç»§ç»­æå–æ•°æ®...")
            
            # ä½¿ç”¨ JavaScript ç›´æ¥æå–æœç´¢ç»“æœï¼ˆæ›´å¯é ï¼‰
            self.logger.info("å¼€å§‹æå–æœç´¢ç»“æœ...")
            
            # æ‰§è¡Œæå–å‡½æ•°ï¼Œä¼ å…¥é…ç½®
            # extractor_js å·²ä»æ–‡ä»¶åŠ è½½ï¼ŒåŒ…å« executeExtraction å‡½æ•°
            js_code = f"{self.extractor_js}\nexecuteExtraction({json.dumps(self.config, ensure_ascii=False)});"
            
            results_data = await page.evaluate(js_code)
            
            # å¤„ç†æå–åˆ°çš„æ•°æ®
            extracted_count = 0
            if results_data and len(results_data) > 0:
                self.logger.info(f"é€šè¿‡ JavaScript æå–åˆ° {len(results_data)} ä¸ªç»“æœ")
                
                for result in results_data:
                    try:
                        item = GoogleSearchItem()
                        item['title'] = result.get('title', '').strip()
                        item['url'] = result.get('url', '').strip()
                        item['description'] = result.get('description', '').strip()
                        item['search_query'] = self.search_query
                        item['page_number'] = page_number
                        item['crawled_at'] = datetime.now().isoformat()
                        
                        # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                        if item['title'] and item['url']:
                            yield item
                            extracted_count += 1
                    except Exception as e:
                        self.logger.warning(f"å¤„ç†ç»“æœæ—¶å‡ºé”™: {e}")
            else:
                self.logger.warning("æœªæå–åˆ°ä»»ä½•ç»“æœï¼")
                # ä¿å­˜é¡µé¢ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                try:
                    # è¯»å–è°ƒè¯•è„šæœ¬
                    debug_js_path = Path(__file__).parent.parent / 'js' / 'debug.js'
                    if debug_js_path.exists():
                        with open(debug_js_path, 'r', encoding='utf-8') as f:
                            debug_js = f.read()
                        # æ‰§è¡Œè°ƒè¯•è„šæœ¬
                        page_info = await page.evaluate(f"{debug_js}\ngetPageInfo();")
                    else:
                        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç®€å•çš„é¡µé¢ä¿¡æ¯
                        if self.utils_js:
                            body_text = (await page.evaluate(f"{self.utils_js}\ngetPageText();"))[:500]
                        else:
                            body_text = (await page.evaluate("document.body.innerText"))[:500]
                        
                        page_info = {
                            "title": await page.title(),
                            "url": page.url,
                            "bodyText": body_text
                        }
                    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä¿å­˜æ—¥å¿—
                    logs_dir = Path(__file__).parent.parent.parent / "logs"
                    logs_dir.mkdir(exist_ok=True)
                    page_info_path = logs_dir / "page_info.json"
                    with open(page_info_path, "w", encoding="utf-8") as f:
                        json.dump(page_info, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"é¡µé¢ä¿¡æ¯å·²ä¿å­˜åˆ°: {page_info_path}")
                except Exception as e:
                    self.logger.error(f"ä¿å­˜é¡µé¢ä¿¡æ¯å¤±è´¥: {e}")
            
            self.logger.info(f"ç¬¬ {page_number} é¡µæˆåŠŸæå–äº† {extracted_count} ä¸ªç»“æœ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            if page_number < self.max_pages and extracted_count > 0:
                next_page_url = None
                
                # æ–¹æ³•1: æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’® - ä»é…ç½®ä¸­è¯»å–
                next_selectors = self.config['selectors']['next_page']['selectors']
                
                for selector in next_selectors:
                    try:
                        next_button = await page.query_selector(selector)
                        if next_button:
                            next_page_url = await next_button.get_attribute('href')
                            if next_page_url:
                                self.logger.info(f"é€šè¿‡é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°ä¸‹ä¸€é¡µ")
                                break
                    except Exception as e:
                        self.logger.debug(f"é€‰æ‹©å™¨ '{selector}' æœªæ‰¾åˆ°: {e}")
                
                # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°æŒ‰é’®ï¼Œå°è¯•ç›´æ¥æ„å»ºä¸‹ä¸€é¡µ URL
                if not next_page_url:
                    try:
                        parsed = urlparse(response.url)
                        params = parse_qs(parsed.query)
                        current_start = int(params.get('start', ['0'])[0])
                        next_start = current_start + 10
                        params['start'] = [str(next_start)]
                        new_query = urlencode(params, doseq=True)
                        next_page_url = urlunparse((
                            parsed.scheme,
                            parsed.netloc,
                            parsed.path,
                            parsed.params,
                            new_query,
                            parsed.fragment
                        ))
                        self.logger.info(f"é€šè¿‡ URL æ„å»ºæ‰¾åˆ°ä¸‹ä¸€é¡µ: {next_page_url}")
                    except Exception as e:
                        self.logger.warning(f"æ„å»ºä¸‹ä¸€é¡µ URL å¤±è´¥: {e}")
                
                # å¦‚æœæ‰¾åˆ°äº†ä¸‹ä¸€é¡µ URLï¼Œç”Ÿæˆè¯·æ±‚
                if next_page_url:
                    if not next_page_url.startswith('http'):
                        next_page_url = response.urljoin(next_page_url)
                    
                    # åœ¨ç¿»é¡µå‰æ·»åŠ é¢å¤–å»¶è¿Ÿï¼Œé™ä½è¯·æ±‚é¢‘ç‡
                    import random
                    page_delay = 10 + random.randint(0, 10)  # 10-20ç§’é¢å¤–å»¶è¿Ÿ
                    self.logger.info(f"ç¿»é¡µå‰ç­‰å¾… {page_delay} ç§’ä»¥é™ä½è¯·æ±‚é¢‘ç‡...")
                    await page.wait_for_timeout(page_delay * 1000)
                    
                    self.logger.info(f"å‡†å¤‡çˆ¬å–ç¬¬ {page_number + 1} é¡µ: {next_page_url}")
                    
                    yield scrapy.Request(
                        url=next_page_url,
                        callback=self.parse,
                        meta={
                            "playwright": True,
                            "playwright_include_page": True,
                            "playwright_page_methods": [
                                PageMethod("wait_for_load_state", "networkidle", timeout=60000),
                            ],
                            "page_number": page_number + 1,
                        },
                        dont_filter=True
                    )
                else:
                    self.logger.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µï¼Œçˆ¬å–å®Œæˆ")
            elif extracted_count == 0:
                self.logger.warning("æœªæå–åˆ°æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ")
            else:
                self.logger.info(f"å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ ({self.max_pages})ï¼Œçˆ¬å–å®Œæˆ")
        
        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢æ—¶å‡ºé”™: {e}", exc_info=True)
            # ä¿å­˜é”™è¯¯ä¿¡æ¯
            try:
                error_info = {
                    "error": str(e),
                    "url": response.url,
                    "page_number": page_number
                }
                # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä¿å­˜é”™è¯¯ä¿¡æ¯
                logs_dir = Path(__file__).parent.parent.parent / "logs"
                logs_dir.mkdir(exist_ok=True)
                error_info_path = logs_dir / "error_info.json"
                with open(error_info_path, "w", encoding="utf-8") as f:
                    json.dump(error_info, f, ensure_ascii=False, indent=2)
            except:
                pass
        
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass
